{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alive-bahrain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in c:\\users\\ct\\documents\\github\\knowledge-hub\\venv\\lib\\site-packages (0.25.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-budapest",
   "metadata": {},
   "source": [
    "# ML/Data Science article 3\n",
    "\n",
    "## Delivering Success in Natural Language Processing Projects: Part Two\n",
    "\n",
    "**Publisher**: *2 articles on Medium -* [*here*](https://medium.com/@ceethinwa/delivering-success-in-natural-language-processing-projects-part-three-79e6e604ddf9) *and* [*here*](https://medium.com/@ceethinwa/delivering-success-in-natural-language-processing-projects-part-three-contd-18fe4fb7ad6d) <br>\n",
    "**Publishing Dates**: *Aug 24, 2022 and Aug 25, 2022*\n",
    "\n",
    "![nlp chatbot](../images/magnifying-glass.svg) <br>\n",
    "*Discovery begins!*\n",
    "\n",
    "*This is the second post of a five-part series where I aim to demystify Natural Language Processing (NLP) through a key \n",
    "learning tool that I would call* **the NLP toolbox**. *You can access the first article\n",
    "[here](https://medium.com/@ceethinwa/delivering-success-in-natural-language-processing-projects-part-one-40c4775cf6a9).*\n",
    "\n",
    "A quick recap:\n",
    "\n",
    "* The NLP toolbox is a framework designed to strike the delicate balance between taking the time to deeply study a phenomenon and rapidly skimming through relevant work\n",
    "* There are 4 stages in the research process where we can apply the NLP toolbox: ***Problem Definition***, ***Data Exploration***, ***Feature Engineering***, and ***Model Fitting and Evaluation***\n",
    "* Consider ***Pain***, ***Accessibility***, ***Specificity***, ***Interest*** and ***Location and Population*** when crafting an NLP problem\n",
    "* Consider the real-world and NLP data technical domains to identify if the problem you are interested in is worth researching\n",
    "* Craft an abstract (mine was about 120 words) that captures the real-world and technical aspects of the problem and the proposed solution to give the big picture\n",
    "\n",
    "**Remember to avoid using a hammer as a tool for everything!**\n",
    "\n",
    "> Not all problems can be or should be solved by NLP solutions.\n",
    "\n",
    "****\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "NLP data is essentially non-visual communications data — it is\n",
    "* Sound waves\n",
    "* Text files\n",
    "\n",
    "Before entering into all the technical ways that we can explore these data types, we first get from the NLP toolbox the tool below:\n",
    "Laughing mouth with LIP (Language Ideas Personas)\n",
    "![LIP framework](../images/LIP-framework.svg)<br>\n",
    "*The LIP framework*\n",
    "\n",
    "* **Language**: The audio or text file will need to be tagged, identifying the language(s), in which the text was written or the voice (recorded in audio) was spoken in\n",
    "* **Ideas**: these represent the themes or topics which the text or audio was based on and/or repeated throughout the text/audio\n",
    "* **Personas**: these are the people and/or organizations associated with the given audio or text.\n",
    "\n",
    "Based on this framework, the following questions need to be answered:\n",
    "\n",
    "![NLP exploration questions](../images/nlp-exploration-questions.svg)<br>\n",
    "*3 types of NLP data exploration questions*\n",
    "\n",
    "Based on the problem that I was working on in my Delta Analytics teaching fellowship, analyzing the `#KOT` Twitter dataset using **LIP** gave the following results:\n",
    "\n",
    "Language\n",
    "\n",
    "> 75.8% of #KOT tweets were tagged by Twitter as English\n",
    ">\n",
    "> Some of the tweets were multilingual e.g. “*I don’t if you have that friend mwenye huwa mnachat na yeye IG, FB na watsapp at the same time na kwa hzo base zote mnaongea story different #KOT*” (English, Kiswahili, Sheng slang)\n",
    "\n",
    "Ideas\n",
    "\n",
    "> Topics are commonly represented on Twitter by hashtags. The most popular hashtags used together with #KOT were:\n",
    ">\n",
    "> 1. `#kenya` (used 3074 times) — a patriotic hashtag\n",
    "> 2. `#loyals` (used 3050 times) — a Twitter-specific hastag inviting people to follow each other\n",
    "> 3. `#nairobi` (used 1196 times) — a city hashtag\n",
    "> 4. `#ikokazike` (used 1016 times) — a career hashtag used by HR and jobseekers in Kenya (it means “There is work KE (Kenya)” in English)\n",
    "> 5. `#bbinonsense` (used 989 times) — a political hashtag used by Kenyans opposed to the [BBI (Building Bridges Initiative)](https://www.muyiconsulting.com/insight/bbi-what-exactly-kenyas-building-bridges-initiative)\n",
    "\n",
    "Personas\n",
    "\n",
    "> The 10 most mentioned ‘persons’ over the past year on Twitter were\n",
    ">\n",
    "> * political institutions such as `@statehousekenya` (mentioned 278 times), `@dcikenya` (mentioned 150 times) and `@nassemblyke` (mentioned 139 times),\n",
    "> * a betting firm, `@safebetske` (mentioned 233 times),\n",
    "> * politicians such as `@railaodinga` (the leader of Kenyan opposition parties, mentioned 205 times) and `@williamsruto` (the current deputy vice president, mentioned 144 times) as well as\n",
    "> * media houses such as `@citizentvkenya` (mentioned 203 times), `@ntvkenya` (mentioned 162 times) and `@classic105kenya` (mentioned 154 times) and\n",
    "> * an inspiration account, `@dodzweit` (A reverend who posts inspirational and at times Christian content, mentioned 197 times)\n",
    ">\n",
    "> A possible explanation for this phenomenon is that 2022 will be an election year, and the two political leaders are running against each other and campaigning online.\n",
    ">\n",
    "> However,\n",
    "> * `@dodzweit` was mentioned only by the church s/he pastors, `@cotchurchhq`, primarily in short summaries of his/her talks.\n",
    "> * `@safebetske` was mentioned only by a forex trader, `@theforexguyke`, primarily in retweets.\n",
    "\n",
    "**We need to keep these questions top of mind even as we get into the more technical ways we will explore text data and audio data.**\n",
    "\n",
    "#### **Exploratory Data Analysis for Text**\n",
    "\n",
    "There are various techniques used to break down text and analyze it. A good number of techniques popular in data science tend to be applied on high-resource languages (languages that are in many freely available texts). To avoid this trap, we will keep it simple and apply them in a manner intuitive as possible in any language.\n",
    "\n",
    "For a computer to “understand” text, it has to be represented as numbers. The process of representing text as numbers is called **encoding**.\n",
    "\n",
    "Text is primarily represented in numerical forms as categorical data. Suppose we want to capture the meaning of an English sentence, “*Hello Mary!*”\n",
    "\n",
    "What if we want to identify *Hello*?\n",
    "\n",
    "![Selecting “Hello” from the same phrase written in Arabic, Chinese and Swahili](../images/hello-written-differently.svg)<br>\n",
    "*All the ways to say “Hello”.*\n",
    "\n",
    "As you can see, these 3 languages couldn’t be further apart in terms of script; “*Hello*” is the first word when written in Chinese and Swahili, but the last word in Arabic.\n",
    "\n",
    "If we express this information as a matrix of binary categories, we get\n",
    "\n",
    "![1 phrase in 3 languages represented in matrix form.](../images/1-phase-in-3-matrices.svg)<br>\n",
    "*All three matrices mean the same.*\n",
    "\n",
    "with all 3 matrices representing the same information, “*Hello Maria*”; because the “*!*” did not carry the message (it only emphasizes tone), it was omitted from the matrices.\n",
    "\n",
    "This is the foundation for much of the data analysis and visualization of text.\n",
    "\n",
    "* Text can be broken down into $x$ documents.\n",
    "* A document can be broken down into $y$ paragraphs.\n",
    "* A paragraph can be broken down into $z$ phrases.\n",
    "* A phrase can be broken down into $n$ words.\n",
    "\n",
    "In frequentist statistics, categorical variables should be represented as\n",
    "\n",
    "* reference category = 0\n",
    "* category2 = 1 when true, 0 when false\n",
    "* category3 = 1 when true, 0 when false\n",
    "\n",
    "etc.\n",
    "\n",
    "This will help draw the following comparisons during analysis:\n",
    "\n",
    "* comparing category 2 with the reference\n",
    "* comparing category 3 with the reference\n",
    "\n",
    "etc.\n",
    "\n",
    "When we look at machine learning, this principle is applied by creating dummy variables (**one-hot encoding**) of keywords of interest where\n",
    "\n",
    "* any other word = 0\n",
    "* keyword 1 = 1 when keyword 1 appears in the text, 0 when it doesn’t\n",
    "* keyword 2 = 1 when keyword 2 appears in the text, 0 when it doesn’t\n",
    "\n",
    "etc.\n",
    "\n",
    "This approach assumes the position of a word in a phrase or sentence has no special meaning, only the word itself. This way of converting text into numbers is called the [**bag of words model**](https://en.wikipedia.org/wiki/Bag-of-words_model) (BOW).\n",
    "\n",
    "BOW is a very common way to analyze monolingual data and as long as selected keywords are representative of the languages in the text, even multilingual data can be analyzed using this approach.\n",
    "\n",
    "However, a weakness of this approach is that a dictionary of filler words (also called [**stop words**](https://en.wikipedia.org/wiki/Stop_word)) must be constructed to strip the text of these words; also short forms must be handled in a uniform way to keep meaning of the root word through [**stemming**](https://en.wikipedia.org/wiki/Stemming) and [**lemmatization**](https://en.wikipedia.org/wiki/Lemmatisation). The text must also be manually labelled; therefore, this approach does not scale well when complex vocabulary and number of words increase.\n",
    "\n",
    "Regarding the measures of central tendency, we can count the number of times a given category appears and get the “**mode**”. Using *Python* libraries like `Spacy` and `NLTK` and *R* libraries like `wordcloud`, `rtweet` and `tidytext` (among others), we can get the frequency of words in a given text and visualize them in the form of a **wordcloud**, where the more frequent a word is, the bigger it is like so:\n",
    "\n",
    "![#KOT wordcloud](https://github.com/CeeThinwa/Delta-Analytics-2021-CT-Project/blob/main/viz1.PNG?raw=true)<br>\n",
    "*Visualization of most common hashtags in my #KOT dataset.*\n",
    "\n",
    "In frequentist statistics, when we relate two categories with one another, they form a [**contingency table**](https://en.wikipedia.org/wiki/Contingency_table), which can give useful information as is. A challenge with words in a text is that there are too many categories to compare, making this form of visualization not very helpful.\n",
    "\n",
    "To compensate, words are grouped in [**n-grams**](https://deepai.org/machine-learning-glossary-and-terms/n-gram) and/or phrases linked together with special punctuation, reducing dimensionality. We can then get the frequency of occurrence of these patterns and report findings like so:\n",
    "\n",
    "![My Little Scraper code](../images/text-analysis-code.png)<br>\n",
    "*This Python code snippet is from: https://github.com/CeeThinwa/MyLittleScraper/blob/master/EDA-Jan2020.ipynb*\n",
    "\n",
    "or visualize the data as follows:\n",
    "\n",
    "![#KOT word fequency bar chart](../images/word-frequency-visual.png)<br>\n",
    "*This R code snippet is from: https://github.com/CeeThinwa/Delta-Analytics-2021-CT-Project/blob/main/KOT%20EDA.ipynb*\n",
    "\n",
    "#### **Exploratory Data Analysis for Audio**\n",
    "\n",
    "Audio tends to be trickier to explore because it is more complex than text. You can’t represent it in easily interpretable numbers like text. As a result, many times, it is easier to transcribe (converting audio to text manually using a human transcriber) audio, then analyze it as text.\n",
    "\n",
    "However, there are steps being taken to analyze audio, particularly speech sounds. In speech, the smallest unit of analysis is the **phoneme**; a word is a combination of one or more phonemes.\n",
    "\n",
    "To visualize sound, it is important to understand the following basic concepts:\n",
    "\n",
    "*     One or more phonemes are contained in a **sound signal**\n",
    "*    A sound signal is represented in a computer as a **sample** — a measurement of the **amplitude** ([how loud your sound is, based on gain and volume](https://www.fxsound.com/blog/what-is-amplitude)) of the sound at fixed time intervals; the number of samples per second is called the **sample rate**\n",
    "*    A sound signal repeats, forming a **waveform** in the process; the **frequency** is the number of waves made by a sound signal in 1 second — the measurement unit of frequency is called the Hertz (Hz)\n",
    "*    Many sound signals at different frequencies are registered by a computer as a **spectrum** — a composite signal representing the real world that is the sum of available frequencies. There are 2 key types of frequencies: the **fundamental frequency** (the one with the lowest Hz) and **harmonics** (whole number multiples of the fundamental frequency).\n",
    "\n",
    "Suppose we want to analyze a particular audio ([listen here](https://ceethinwa.github.io/resources/aud/Abstract.mp3)). The first step is to convert this file from `.mp3` to `.wav` to because the former format includes compression and it is more complex. The code to do so is here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "secret-ground",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydub'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-745a2f88f212>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# import required modules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpydub\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# assign files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pydub'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In your Linux command line, run the following commands first:\n",
    "sudo apt-get update\n",
    "sudo apt-get install ffmpeg\n",
    "sudo apt-get install frei0r-plugins\n",
    "Once this is done, run the Python commands below:\n",
    "\"\"\"\n",
    "\n",
    "# import required modules\n",
    "from os import path\n",
    "from pydub import AudioSegment\n",
    "  \n",
    "# assign files\n",
    "input_file = \"resources_aud_Abstract.mp3\"\n",
    "output_file = \"resources_aud_Abstract.wav\"\n",
    "\n",
    "# convert mp3 file to wav file\n",
    "sound = AudioSegment.from_mp3(input_file)\n",
    "sound.export(output_file, format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-chess",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "Further reading resources:\n",
    "1. **Article 1**\n",
    "    * [Exploratory Data Analysis for Text Data](https://dair.ai/Exploratory_Data_Analysis_for_Text_Data/)\n",
    "    * [A Beginner’s Guide to Exploratory Data Analysis (EDA) on Text Data (Amazon Case Study)](https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/')\n",
    "    * [Exploratory Data Analysis of Text Data: employee reviews](https://towardsdatascience.com/nlp-part-3-exploratory-data-analysis-of-text-data-1caa8ab3f79d)\n",
    "    * [A Complete Exploratory Data Analysis and Visualization for Text Data](https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a)\n",
    "    * [Chi-Square (Χ²) Tests | Types, Formula & Examples](https://www.scribbr.com/statistics/chi-square-tests/)\n",
    "    * [Contingency Table: Definition, Examples & Interpreting](https://statisticsbyjim.com/basics/contingency-table/)\n",
    "    * [My Little Scraper](https://github.com/CeeThinwa/MyLittleScraper/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
